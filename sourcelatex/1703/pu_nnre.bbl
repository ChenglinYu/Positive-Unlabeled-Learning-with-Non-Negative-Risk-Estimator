\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Denis(1998)]{denis98alt}
F.~Denis.
\newblock {PAC} learning from positive statistical queries.
\newblock In \emph{ALT}, 1998.

\bibitem[{De Comit\'{e}} et~al.(1999){De Comit\'{e}}, Denis, Gilleron, and
  Letouzey]{comite99alt}
F.~{De Comit\'{e}}, F.~Denis, R.~Gilleron, and F.~Letouzey.
\newblock Positive and unlabeled examples help learning.
\newblock In \emph{ALT}, 1999.

\bibitem[Letouzey et~al.(2000)Letouzey, Denis, and Gilleron]{letouzey00alt}
F.~Letouzey, F.~Denis, and R.~Gilleron.
\newblock Learning from positive and unlabeled examples.
\newblock In \emph{ALT}, 2000.

\bibitem[Elkan and Noto(2008)]{elkan08kdd}
C.~Elkan and K.~Noto.
\newblock Learning classifiers from only positive and unlabeled data.
\newblock In \emph{KDD}, 2008.

\bibitem[Ward et~al.(2009)Ward, Hastie, Barry, Elith, and
  Leathwick]{ward09biometrics}
G.~Ward, T.~Hastie, S.~Barry, J.~Elith, and J.~Leathwick.
\newblock Presence-only data and the {EM} algorithm.
\newblock \emph{Biometrics}, 65\penalty0 (2):\penalty0 554--563, 2009.

\bibitem[Scott and Blanchard(2009)]{scott09aistats}
C.~Scott and G.~Blanchard.
\newblock Novelty detection: Unlabeled data definitely help.
\newblock In \emph{AISTATS}, 2009.

\bibitem[Blanchard et~al.(2010)Blanchard, Lee, and Scott]{blanchard10jmlr}
G.~Blanchard, G.~Lee, and C.~Scott.
\newblock Semi-supervised novelty detection.
\newblock \emph{Journal of Machine Learning Research}, 11:\penalty0 2973--3009,
  2010.

\bibitem[Hsieh et~al.(2015)Hsieh, Natarajan, and Dhillon]{hsieh15icml}
C.-J. Hsieh, N.~Natarajan, and I.~S. Dhillon.
\newblock {PU} learning for matrix completion.
\newblock In \emph{ICML}, 2015.

\bibitem[Li et~al.(2009)Li, Yu, Liu, and Ng]{li09sdm}
X.~Li, P.~S. Yu, B.~Liu, and S.-K. Ng.
\newblock Positive unlabeled learning for data stream classification.
\newblock In \emph{SDM}, 2009.

\bibitem[Nguyen et~al.(2011)Nguyen, Li, and Ng]{nguyen11ijcai}
M.~N. Nguyen, X.~Li, and S.-K. Ng.
\newblock Positive unlabeled leaning for time series classification.
\newblock In \emph{IJCAI}, 2011.

\bibitem[Liu et~al.(2002)Liu, Lee, Yu, and Li]{liu02icml}
B.~Liu, W.~S. Lee, P.~S. Yu, and X.~Li.
\newblock Partially supervised classification of text documents.
\newblock In \emph{ICML}, 2002.

\bibitem[Li and Liu(2003)]{li03ijcai}
X.~Li and B.~Liu.
\newblock Learning to classify texts using positive and unlabeled data.
\newblock In \emph{IJCAI}, 2003.

\bibitem[Lee and Liu(2003)]{lee03icml}
W.~S. Lee and B.~Liu.
\newblock Learning with positive and unlabeled examples using weighted logistic
  regression.
\newblock In \emph{ICML}, 2003.

\bibitem[Liu et~al.(2003)Liu, Dai, Li, Lee, and Yu.]{liu03icdm}
B.~Liu, Y.~Dai, X.~Li, W.~S. Lee, and P.~S. Yu.
\newblock Building text classifiers using positive and unlabeled examples.
\newblock In \emph{ICDM}, 2003.

\bibitem[{du Plessis} et~al.(2014){du Plessis}, Niu, and
  Sugiyama]{christo14nips}
M.~C. {du Plessis}, G.~Niu, and M.~Sugiyama.
\newblock Analysis of learning from positive and unlabeled data.
\newblock In \emph{NIPS}, 2014.

\bibitem[{du Plessis} et~al.(2015){du Plessis}, Niu, and
  Sugiyama]{christo15icml}
M.~C. {du Plessis}, G.~Niu, and M.~Sugiyama.
\newblock Convex formulation for learning from positive and unlabeled data.
\newblock In \emph{ICML}, 2015.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and
  Tewari]{natarajan13nips}
N.~Natarajan, I.~S. Dhillon, P.~Ravikumar, and A.~Tewari.
\newblock Learning with noisy labels.
\newblock In \emph{NIPS}, 2013.

\bibitem[Patrini et~al.(2016)Patrini, Nielsen, Nock, and
  Carioni]{patrini16icml}
G.~Patrini, F.~Nielsen, R.~Nock, and M.~Carioni.
\newblock Loss factorization, weakly supervised learning and label noise
  robustness.
\newblock In \emph{ICML}, 2016.

\bibitem[Niu et~al.(2016)Niu, {du Plessis}, Sakai, Ma, and Sugiyama]{niu16nips}
G.~Niu, M.~C. {du Plessis}, T.~Sakai, Y.~Ma, and M.~Sugiyama.
\newblock Theoretical comparisons of positive-unlabeled learning against
  positive-negative learning.
\newblock In \emph{NIPS}, 2016.

\bibitem[Kingma and Ba(2015)]{kingma15iclr}
D.~P. Kingma and J.~L. Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Sansone et~al.(2016)Sansone, {De Natale}, and Zhou]{sansone16arxiv}
E.~Sansone, F.~G.~B. {De Natale}, and Z.-H. Zhou.
\newblock Efficient training for positive unlabeled learning.
\newblock \emph{arXiv preprint arXiv:1608.06807}, 2016.

\bibitem[Platt(1999)]{platt99SMO}
J.~C. Platt.
\newblock Fast training of support vector machines using sequential minimal
  optimization.
\newblock In B.~Sch\"{o}lkopf, C.~J.~C. Burges, and A.~J. Smola, editors,
  \emph{Advances in Kernel Methods}, pages 185--208. MIT Press, 1999.

\bibitem[A.~Menon(2015)]{menon15icml}
C.~S. Ong B.~Williamson A.~Menon, B.~{Van Rooyen}.
\newblock Learning from corrupted binary labels via class-probability
  estimation.
\newblock In \emph{ICML}, 2015.

\bibitem[Ramaswamy et~al.(2016)Ramaswamy, Scott, and Tewari]{ramaswamy16icml}
H.~G. Ramaswamy, C.~Scott, and A.~Tewari.
\newblock Mixture proportion estimation via kernel embedding of distributions.
\newblock In \emph{ICML}, 2016.

\bibitem[Jain et~al.(2016)Jain, White, and Radivojac]{jain16nips}
S.~Jain, M.~White, and P.~Radivojac.
\newblock Estimating the class prior and posterior from noisy positives and
  unlabeled data.
\newblock In \emph{NIPS}, 2016.

\bibitem[{du Plessis} et~al.(2017){du Plessis}, Niu, and
  Sugiyama]{christo17mlj}
M.~C. {du Plessis}, G.~Niu, and M.~Sugiyama.
\newblock Class-prior estimation for learning from positive and unlabeled data.
\newblock \emph{Machine Learning}, 106\penalty0 (4):\penalty0 463--492, 2017.

\bibitem[Bartlett et~al.(2006)Bartlett, Jordan, and McAuliffe]{bartlett06jasa}
P.~L. Bartlett, M.~I. Jordan, and J.~D. McAuliffe.
\newblock Convexity, classification, and risk bounds.
\newblock \emph{Journal of the American Statistical Association}, 101\penalty0
  (473):\penalty0 138--156, 2006.

\bibitem[Mohri et~al.(2012)Mohri, Rostamizadeh, and Talwalkar]{mohri12FML}
M.~Mohri, A.~Rostamizadeh, and A.~Talwalkar.
\newblock \emph{Foundations of Machine Learning}.
\newblock MIT Press, 2012.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun98mnist}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Yuille and Rangarajan(2001)]{yuille01nips}
A.~L. Yuille and A.~Rangarajan.
\newblock The concave-convex procedure ({CCCP}).
\newblock In \emph{NIPS}, 2001.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi11jmlr}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121--2159,
  2011.

\bibitem[Chung(1968)]{chung68CPT}
K.-L. Chung.
\newblock \emph{A Course in Probability Theory}.
\newblock Academic Press, 1968.

\bibitem[Stein(1956)]{stein56bsmsp}
C.~Stein.
\newblock Inadmissibility of the usual estimator for the mean of a multivariate
  normal distribution.
\newblock In \emph{Proc. 3rd Berkeley Symposium on Mathematical Statistics and
  Probability}, 1956.

\bibitem[James and Stein(1961)]{james61bsmsp}
W.~James and C.~Stein.
\newblock Estimation with quadratic loss.
\newblock In \emph{Proc. 4th Berkeley Symposium on Mathematical Statistics and
  Probability}, 1961.

\bibitem[Koltchinskii(2001)]{koltchinskii01tit}
V.~Koltchinskii.
\newblock Rademacher penalties and structural risk minimization.
\newblock \emph{IEEE Transactions on Information Theory}, 47\penalty0
  (5):\penalty0 1902--1914, 2001.

\bibitem[Bartlett and Mendelson(2002)]{bartlett02jmlr}
P.~L. Bartlett and S.~Mendelson.
\newblock Rademacher and {Gaussian} complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3:\penalty0 463--482,
  2002.

\bibitem[Yuan et~al.(2012)Yuan, Ho, and Lin]{yuan12epsilon}
G.-X. Yuan, C.-H. Ho, and C.-J. Lin.
\newblock An improved {GLMNET} for l1-regularized logistic regression.
\newblock \emph{Journal of Machine Learning Research}, 13:\penalty0 1999--2030,
  2012.

\bibitem[Lang(1995)]{lang95icml}
K.~Lang.
\newblock Newsweeder: Learning to filter netnews.
\newblock In \emph{ICML}, 1995.

\bibitem[Krizhevsky(2009)]{krizhevsky09cifar}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Nair and Hinton(2010)]{nair10icml}
V.~Nair and G.~E. Hinton.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In \emph{ICML}, 2010.

\bibitem[Glorot and Bengio(2010)]{glorot10aistats}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{AISTATS}, 2010.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and
  Manning]{pennington14emnlp}
J.~Pennington, R.~Socher, and C.~D. Manning.
\newblock {GloVe}: Global vectors for word representation.
\newblock In \emph{EMNLP}, 2014.

\bibitem[Springenberg et~al.(2015)Springenberg, Dosovitskiy, Brox, and
  Riedmiller]{springenberg15iclr}
J.~T. Springenberg, A.~Dosovitskiy, T.~Brox, and M.~Riedmiller.
\newblock Striving for simplicity: The all convolutional net.
\newblock In \emph{ICLR}, 2015.

\bibitem[Ioffe and Szegedy(2015)]{ioffe15icml}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{ICML}, 2015.

\bibitem[Tokui et~al.(2015)Tokui, Oono, Hido, and Clayton]{tokui15mlsys}
S.~Tokui, K.~Oono, S.~Hido, and J.~Clayton.
\newblock Chainer: a next-generation open source framework for deep learning.
\newblock In \emph{Machine Learning Systems Workshop at NIPS}, 2015.

\bibitem[Sakai et~al.(2017)Sakai, {du Plessis}, Niu, and Sugiyama]{sakai17icml}
T.~Sakai, M.~C. {du Plessis}, G.~Niu, and M.~Sugiyama.
\newblock Semi-supervised classification based on classification from positive
  and unlabeled data.
\newblock In \emph{ICML}, 2017.

\bibitem[McDiarmid(1989)]{mcdiarmid89MBD}
C.~McDiarmid.
\newblock On the method of bounded differences.
\newblock In J.~Siemons, editor, \emph{Surveys in Combinatorics}, pages
  148--188. Cambridge University Press, 1989.

\bibitem[Ledoux and Talagrand(1991)]{ledoux91PBS}
M.~Ledoux and M.~Talagrand.
\newblock \emph{Probability in Banach Spaces: Isoperimetry and Processes}.
\newblock Springer, 1991.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{ssbd14UML}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock \emph{Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem[Vapnik(1998)]{vapnik98SLT}
V.~N. Vapnik.
\newblock \emph{Statistical Learning Theory}.
\newblock John Wiley \& Sons, 1998.

\end{thebibliography}
